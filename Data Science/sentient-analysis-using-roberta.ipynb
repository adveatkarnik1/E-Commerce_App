{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-23T17:48:42.829952Z","iopub.execute_input":"2023-10-23T17:48:42.830679Z","iopub.status.idle":"2023-10-23T17:48:55.340574Z","shell.execute_reply.started":"2023-10-23T17:48:42.830646Z","shell.execute_reply":"2023-10-23T17:48:55.339423Z"},"editable":false,"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom transformers import AutoTokenizer, RobertaTokenizer, AdamWeightDecay, TFAutoModel\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:48:55.342706Z","iopub.execute_input":"2023-10-23T17:48:55.343032Z","iopub.status.idle":"2023-10-23T17:49:04.989793Z","shell.execute_reply.started":"2023-10-23T17:48:55.343004Z","shell.execute_reply":"2023-10-23T17:49:04.988720Z"},"editable":false,"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:04.991155Z","iopub.execute_input":"2023-10-23T17:49:04.991688Z","iopub.status.idle":"2023-10-23T17:49:05.401400Z","shell.execute_reply.started":"2023-10-23T17:49:04.991660Z","shell.execute_reply":"2023-10-23T17:49:05.400252Z"},"editable":false,"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Download NLTK resources\nnltk.download('punkt')  # Download the NLTK resource for tokenization\n\n# Unzip WordNet data file to the NLTK data directory\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n# WordNet is a lexical database for the English language. This command unzips its data to the appropriate NLTK data directory.\n\nnltk.download('wordnet')  # Download the NLTK resource for WordNet\n# WordNet is a large lexical database of English. It's used for various NLP tasks, including lemmatization and synonym/antonym retrieval.\n\nnltk.download('stopwords')  # Download the NLTK resource for stopwords\n# Stopwords are common words (e.g., \"the,\" \"and,\" \"is\") that are often removed from text data when performing natural language processing tasks.\n\nstop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n# This line initializes a set containing common English stopwords, which can be used for text preprocessing.\n\nlemmatizer = WordNetLemmatizer()  # Initialize a WordNet lemmatizer\n# The WordNet lemmatizer is used to reduce words to their base or dictionary form. For example, \"running\" would be lemmatized to \"run.\"\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:05.402836Z","iopub.execute_input":"2023-10-23T17:49:05.403257Z","iopub.status.idle":"2023-10-23T17:49:06.848249Z","shell.execute_reply.started":"2023-10-23T17:49:05.403217Z","shell.execute_reply":"2023-10-23T17:49:06.847147Z"},"editable":false,"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/cumulative-reviews-and-ratings/cumulative.csv\")\ndataset = dataset[dataset['rating'] != '|']\n#make 100000 instead if 10000\ndataset = dataset.sample(n=10000, random_state=42) #Taking a sample for training, otherwise kernel crashes, ig for offline implementation, this shouldn't be an issue","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:06.850986Z","iopub.execute_input":"2023-10-23T17:49:06.851261Z","iopub.status.idle":"2023-10-23T17:49:12.892522Z","shell.execute_reply.started":"2023-10-23T17:49:06.851237Z","shell.execute_reply":"2023-10-23T17:49:12.891455Z"},"editable":false,"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/2306721726.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  dataset=pd.read_csv(\"/kaggle/input/cumulative-reviews-and-ratings/cumulative.csv\")\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(dataset['review'].iloc[0]))","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:12.893960Z","iopub.execute_input":"2023-10-23T17:49:12.894231Z","iopub.status.idle":"2023-10-23T17:49:12.899996Z","shell.execute_reply.started":"2023-10-23T17:49:12.894207Z","shell.execute_reply":"2023-10-23T17:49:12.898914Z"},"editable":false,"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['review'] = dataset['review'].str.replace('&', 'and')\n\ndef remove_urls(text):\n    # Regular expression to match URLs\n    url_pattern = r'https?://\\S+|www\\.\\S+'\n    return re.sub(url_pattern, '', text)\n\n# Apply the remove_urls function to the 'review' column\ndataset['review'] = dataset['review'].apply(lambda x: remove_urls(x))\n\ndef remove_non_alphanumeric(text):\n    return re.sub(r'[^a-zA-Z0-9.]', ' ', text)\n\n# Apply the remove_non_alphanumeric function to the \"reviews\" column\ndataset['review'] = dataset['review'].apply(remove_non_alphanumeric)\n\nprint(list(dataset.columns))\nprint(dataset.head())\nprint(len(dataset))","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:12.900909Z","iopub.execute_input":"2023-10-23T17:49:12.901239Z","iopub.status.idle":"2023-10-23T17:49:13.166955Z","shell.execute_reply.started":"2023-10-23T17:49:12.901214Z","shell.execute_reply":"2023-10-23T17:49:13.165994Z"},"editable":false,"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['rating', 'review']\n       rating                                             review\n303990      5  My son and I love these beef sticks  I purchas...\n269665      5  Since the stores do not carry this product  I ...\n380357      5  My cat loves all the Weruva varieties. I rotat...\n37894       5  I can have about 3 of these a week as a protei...\n444547      5  HAVE BEEN USING THE CROWN PRICE LUMP WHITE CRA...\n5000\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['rating'] = dataset['rating'].astype(float).round() #converting ratings to numerical values\ndataset['review'] = dataset['review'].str.lower() #uniformly making all letters lower case in the reviews\n\nprint(list(dataset.columns))\nprint(dataset.head())\nprint(len(dataset))","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:13.168053Z","iopub.execute_input":"2023-10-23T17:49:13.168329Z","iopub.status.idle":"2023-10-23T17:49:13.189982Z","shell.execute_reply.started":"2023-10-23T17:49:13.168306Z","shell.execute_reply":"2023-10-23T17:49:13.189047Z"},"editable":false,"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['rating', 'review']\n        rating                                             review\n303990     5.0  my son and i love these beef sticks  i purchas...\n269665     5.0  since the stores do not carry this product  i ...\n380357     5.0  my cat loves all the weruva varieties. i rotat...\n37894      5.0  i can have about 3 of these a week as a protei...\n444547     5.0  have been using the crown price lump white cra...\n5000\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef lemmatize_and_remove_stopwords(text):\n    words = nltk.word_tokenize(text) \n    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    return ' '.join(filtered_words)\n\n# Apply the lemmatize_and_remove_stopwords function to the \"reviews\" column\ndataset['review'] = dataset['review'].apply(lemmatize_and_remove_stopwords)\n\nprint(list(dataset.columns))\nprint(dataset.head())\nprint(len(dataset))\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:13.191159Z","iopub.execute_input":"2023-10-23T17:49:13.191450Z","iopub.status.idle":"2023-10-23T17:49:21.359737Z","shell.execute_reply.started":"2023-10-23T17:49:13.191413Z","shell.execute_reply":"2023-10-23T17:49:21.358766Z"},"editable":false,"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['rating', 'review']\n        rating                                             review\n303990     5.0  son love beef stick purchased slim jims time a...\n269665     5.0  since store carry product lucky come across li...\n380357     5.0  cat love weruva variety . rotate wellness wet ...\n37894      5.0  3 week protein meal snack anytime feeling hung...\n444547     5.0  using crown price lump white crab meat years.i...\n5000\n","output_type":"stream"}]},{"cell_type":"code","source":"q=list(dataset['review'].isna())\nprint(sum(q))","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.360937Z","iopub.execute_input":"2023-10-23T17:49:21.361228Z","iopub.status.idle":"2023-10-23T17:49:21.367607Z","shell.execute_reply.started":"2023-10-23T17:49:21.361203Z","shell.execute_reply":"2023-10-23T17:49:21.366605Z"},"editable":false,"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"# #LOOK INTO THIS MAX_LENGTH AND WHAT ARE THE PARAMETERS I AM USING\n# def to_tokens(input_text, tokenizer):\n#     output = tokenizer.encode_plus(text=input_text, max_length=90, pad_to_max_length=True, truncation=True)\n# #     print(\"to_tokens:\", type(output))\n# #     print(output.keys())\n#     return output\n\n# def select_field(features, field):\n#     return [feature[field] for feature in features]","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.368707Z","iopub.execute_input":"2023-10-23T17:49:21.368996Z","iopub.status.idle":"2023-10-23T17:49:21.380636Z","shell.execute_reply.started":"2023-10-23T17:49:21.368963Z","shell.execute_reply":"2023-10-23T17:49:21.379733Z"},"editable":false,"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# string=\"I love NLP, I need to know more about it. He is happy where he is.\"\n# print(string)\n# string = remove_non_alphanumeric(string)\n# print(string)\n# string = lemmatize_and_remove_stopwords(string)\n# string=string.lower()\n# print(string)\n\n# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# out = to_tokens(string,tokenizer)\n\n# print(len(out['input_ids']),len(out['attention_mask']))\n# print(out['input_ids'])\n# print(out['attention_mask'])\n\n# print(tokenizer.convert_ids_to_tokens(out['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.381714Z","iopub.execute_input":"2023-10-23T17:49:21.382029Z","iopub.status.idle":"2023-10-23T17:49:21.392525Z","shell.execute_reply.started":"2023-10-23T17:49:21.382004Z","shell.execute_reply":"2023-10-23T17:49:21.391642Z"},"editable":false,"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"input_text = \"I love natural language processing\"\n\nsimplified output =\n{\n    'input_ids': [0, 101, 1045, 2293, 3015, 2652, 3836, 102, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n}","metadata":{"editable":false}},{"cell_type":"code","source":"# def preprocess_data(tokenizer):\n#     data_encoded = dataset['review'].apply(lambda x: to_tokens(x, tokenizer))\n\n#     # Create attention masks\n#     input_ids = np.array(select_field(data_encoded, 'input_ids'))\n#     attention_masks = np.array(select_field(data_encoded, 'attention_mask'))\n    \n#     # Manually split data into train and test sets\n#     split_index = int(len(data_encoded) * 0.7)\n#     X_train_input_ids, X_test_input_ids = input_ids[:split_index], input_ids[split_index:]\n#     X_train_attention_masks, X_test_attention_masks = attention_masks[:split_index], attention_masks[split_index:]\n    \n#     X_train = np.array([X_train_input_ids, X_train_attention_masks])\n#     X_test = np.array([X_test_input_ids, X_test_attention_masks])\n    \n#     y_train = np.array(dataset['rating'].iloc[:split_index])\n#     y_test = np.array(dataset['rating'].iloc[split_index:])\n    \n#     return X_train, X_test, y_train, y_test\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.393586Z","iopub.execute_input":"2023-10-23T17:49:21.393862Z","iopub.status.idle":"2023-10-23T17:49:21.407422Z","shell.execute_reply.started":"2023-10-23T17:49:21.393838Z","shell.execute_reply":"2023-10-23T17:49:21.406458Z"},"editable":false,"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def load(learning_rate,epsilon=1e-8,num_labels=5,task=\"sentiment_analysis\",model_name='roberta-large'):\n    config_class, model_class, tokenizer_class= RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer\n\n    config = config_class.from_pretrained(model_name, num_labels=num_labels, finetuning_task=\"multi_class\")\n\n\n    model_metrics = [\n        keras.metrics.TruePositives(name='tp'),\n        keras.metrics.FalsePositives(name='fp'), \n        keras.metrics.TrueNegatives(name='tn'),\n        keras.metrics.FalseNegatives(name='fn'), \n        keras.metrics.BinaryAccuracy(name='accuracy'),\n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name='auc'),\n    ]\n\n    model = model_class.from_pretrained(model_name)\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=1.0)\n    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n    metric = keras.metrics.CategoricalAccuracy('accuracy')\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n    tokenizer = tokenizer_class.from_pretrained(model_name, lower_case = False)\n    return config, model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.412335Z","iopub.execute_input":"2023-10-23T17:49:21.412606Z","iopub.status.idle":"2023-10-23T17:49:21.421492Z","shell.execute_reply.started":"2023-10-23T17:49:21.412583Z","shell.execute_reply":"2023-10-23T17:49:21.420650Z"},"editable":false,"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# config, model, tokenizer = load(learning_rate=2e-5)\n# X_train,X_test,y_train,y_test=preprocess_data(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.422511Z","iopub.execute_input":"2023-10-23T17:49:21.422759Z","iopub.status.idle":"2023-10-23T17:49:21.436326Z","shell.execute_reply.started":"2023-10-23T17:49:21.422736Z","shell.execute_reply":"2023-10-23T17:49:21.435442Z"},"editable":false,"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train = dataset.iloc[:int(dataset.shape[0] * 0.7)]\ntest = dataset.iloc[int(dataset.shape[0] * 0.7):]\nprint(train.shape,test.shape)\nx_train,y_train = train['review'],train['rating']\nx_test, y_test = test['review'],test['rating']\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.437507Z","iopub.execute_input":"2023-10-23T17:49:21.437864Z","iopub.status.idle":"2023-10-23T17:49:21.448875Z","shell.execute_reply.started":"2023-10-23T17:49:21.437832Z","shell.execute_reply":"2023-10-23T17:49:21.447913Z"},"editable":false,"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(3500, 2) (1500, 2)\n(3500,) (3500,)\n(1500,) (1500,)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer =  RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef tokenize(df):\n    input_ids =  []\n    attention_masks =  []\n    \n    for i, text in enumerate(df):\n        tokens = tokenizer.encode_plus(text, max_length=70,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_attention_mask=True,\n                                   return_token_type_ids=False, return_tensors='tf')\n         \n        input_ids.append(np.asarray(tokens[\"input_ids\"]).reshape(70,))\n        attention_masks.append(np.asarray(tokens[\"attention_mask\"]).reshape(70,))\n\n    return (np.asarray(input_ids), np.asarray(attention_masks))\n\nx_train_input_ids, x_train_attention_masks = tokenize(x_train)\nx_test_input_ids, x_test_attention_masks = tokenize(x_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:21.450042Z","iopub.execute_input":"2023-10-23T17:49:21.450383Z","iopub.status.idle":"2023-10-23T17:49:29.049726Z","shell.execute_reply.started":"2023-10-23T17:49:21.450350Z","shell.execute_reply":"2023-10-23T17:49:29.048782Z"},"editable":false,"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a138be93c714ac6ac40996e37ffa8cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8470a509299941cea2497b97d83d751a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3347f5adcce44e859cac8a2e5afb0767"}},"metadata":{}}]},{"cell_type":"code","source":"print(x_train_input_ids.shape, x_train_attention_masks.shape)\nprint(x_test_input_ids.shape, x_test_attention_masks.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:29.051631Z","iopub.execute_input":"2023-10-23T17:49:29.051977Z","iopub.status.idle":"2023-10-23T17:49:29.056826Z","shell.execute_reply.started":"2023-10-23T17:49:29.051941Z","shell.execute_reply":"2023-10-23T17:49:29.055925Z"},"editable":false,"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(3500, 45) (3500, 45)\n(1500, 45) (1500, 45)\n","output_type":"stream"}]},{"cell_type":"code","source":"def one_encode_labels(df):\n    sentiment_values = set(dataset[\"rating\"].values)\n    labels = []\n    for index, row in df.iterrows():\n        label = np.zeros((len(sentiment_values)))\n        label[row[\"sentiment\"]] = 1 \n        labels.append(label)\n    labels = np.asarray(labels)\n    return labels","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:29.057894Z","iopub.execute_input":"2023-10-23T17:49:29.058204Z","iopub.status.idle":"2023-10-23T17:49:29.068581Z","shell.execute_reply.started":"2023-10-23T17:49:29.058180Z","shell.execute_reply":"2023-10-23T17:49:29.067777Z"},"editable":false,"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:29.069891Z","iopub.execute_input":"2023-10-23T17:49:29.070235Z","iopub.status.idle":"2023-10-23T17:49:29.082174Z","shell.execute_reply.started":"2023-10-23T17:49:29.070204Z","shell.execute_reply":"2023-10-23T17:49:29.081328Z"},"editable":false,"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"303990    5.0\n269665    5.0\n380357    5.0\n37894     5.0\n444547    5.0\n         ... \n218888    5.0\n461940    5.0\n384366    5.0\n27587     5.0\n589107    5.0\nName: rating, Length: 3500, dtype: float64 558557    2.0\n534247    5.0\n528472    5.0\n335240    5.0\n567941    3.0\n         ... \n625724    5.0\n508752    3.0\n207380    5.0\n129907    5.0\n419905    5.0\nName: rating, Length: 1500, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:29.083604Z","iopub.execute_input":"2023-10-23T17:49:29.083877Z","iopub.status.idle":"2023-10-23T17:49:29.095206Z","shell.execute_reply.started":"2023-10-23T17:49:29.083854Z","shell.execute_reply":"2023-10-23T17:49:29.094293Z"},"editable":false,"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"303990    5.0\n269665    5.0\n380357    5.0\n37894     5.0\n444547    5.0\n         ... \n218888    5.0\n461940    5.0\n384366    5.0\n27587     5.0\n589107    5.0\nName: rating, Length: 3500, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"def one_encode_labels(df):\n    sentiment_values = set(df)\n    labels = []\n    for rating in df:\n        label = np.zeros((len(sentiment_values)))\n        if rating == 1.0:\n            label[0]=1\n        elif rating == 2.0:\n            label[1] =1\n        elif rating ==3.0:\n            label[2]=1\n        elif rating == 4.0:\n            label[3]=1\n        elif rating == 5.0:\n            label[4] =1\n        labels.append(label)\n    labels = np.asarray(labels)\n    return labels\n\ny_train = one_encode_labels(y_train)\ny_test = one_encode_labels(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:29.096486Z","iopub.execute_input":"2023-10-23T17:49:29.096794Z","iopub.status.idle":"2023-10-23T17:49:29.116877Z","shell.execute_reply.started":"2023-10-23T17:49:29.096763Z","shell.execute_reply":"2023-10-23T17:49:29.116082Z"},"editable":false,"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"roberta = TFAutoModel.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:29.118004Z","iopub.execute_input":"2023-10-23T17:49:29.118325Z","iopub.status.idle":"2023-10-23T17:49:40.044657Z","shell.execute_reply.started":"2023-10-23T17:49:29.118301Z","shell.execute_reply":"2023-10-23T17:49:40.043862Z"},"editable":false,"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e437db153b4b2ea1010bbb5a950108"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = tf.keras.layers.Input(shape=(70,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(70,), name='attention_mask', dtype='int32')\n\nembeddings = roberta(input_ids, attention_mask=mask)[0]\n\nX = tf.keras.layers.LSTM(128)(embeddings)\nX = tf.keras.layers.BatchNormalization()(X)\nX = tf.keras.layers.Dense(768)(X)\nX = tf.keras.layers.Activation(\"relu\")(X)\nX = tf.keras.layers.Dense(768)(X)\nX = tf.keras.layers.Dropout(0.1)(X)\ny = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(X)\n\nmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\nmodel.layers[2].trainable = False\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:40.045932Z","iopub.execute_input":"2023-10-23T17:49:40.046315Z","iopub.status.idle":"2023-10-23T17:49:47.188809Z","shell.execute_reply.started":"2023-10-23T17:49:40.046280Z","shell.execute_reply":"2023-10-23T17:49:47.187913Z"},"editable":false,"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 45)]         0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 45)]         0           []                               \n                                                                                                  \n tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n el)                            thPoolingAndCrossAt               'attention_mask[0][0]']         \n                                tentions(last_hidde                                               \n                                n_state=(None, 45,                                                \n                                768),                                                             \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n lstm (LSTM)                    (None, 128)          459264      ['tf_roberta_model[0][0]']       \n                                                                                                  \n batch_normalization (BatchNorm  (None, 128)         512         ['lstm[0][0]']                   \n alization)                                                                                       \n                                                                                                  \n dense (Dense)                  (None, 768)          99072       ['batch_normalization[0][0]']    \n                                                                                                  \n activation (Activation)        (None, 768)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 768)          590592      ['activation[0][0]']             \n                                                                                                  \n dropout_37 (Dropout)           (None, 768)          0           ['dense_1[0][0]']                \n                                                                                                  \n outputs (Dense)                (None, 5)            3845        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 125,798,917\nTrainable params: 1,153,029\nNon-trainable params: 124,645,888\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = AdamWeightDecay(2e-03, beta_1=0.8, beta_2=0.9, weight_decay_rate=0.0001)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[acc])","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:47.190105Z","iopub.execute_input":"2023-10-23T17:49:47.190384Z","iopub.status.idle":"2023-10-23T17:49:47.214814Z","shell.execute_reply.started":"2023-10-23T17:49:47.190359Z","shell.execute_reply":"2023-10-23T17:49:47.214079Z"},"editable":false,"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(x_train_input_ids.shape, x_train_attention_masks.shape)\nprint(x_test_input_ids.shape,x_test_attention_masks.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:47.215816Z","iopub.execute_input":"2023-10-23T17:49:47.216106Z","iopub.status.idle":"2023-10-23T17:49:47.221196Z","shell.execute_reply.started":"2023-10-23T17:49:47.216081Z","shell.execute_reply":"2023-10-23T17:49:47.220290Z"},"editable":false,"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(3500, 45) (3500, 45)\n(1500, 45) (1500, 45)\n(3500, 5) (1500, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n# assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:47.222370Z","iopub.execute_input":"2023-10-23T17:49:47.222632Z","iopub.status.idle":"2023-10-23T17:49:47.231910Z","shell.execute_reply.started":"2023-10-23T17:49:47.222609Z","shell.execute_reply":"2023-10-23T17:49:47.231047Z"},"editable":false,"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"history = model.fit((x_train_input_ids, x_train_attention_masks), y_train, validation_data=((x_test_input_ids, x_test_attention_masks), y_test), epochs=10)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:49:47.232865Z","iopub.execute_input":"2023-10-23T17:49:47.233159Z","iopub.status.idle":"2023-10-23T17:52:09.118289Z","shell.execute_reply.started":"2023-10-23T17:49:47.233135Z","shell.execute_reply":"2023-10-23T17:52:09.117271Z"},"editable":false,"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/10\n110/110 [==============================] - 31s 148ms/step - loss: 1.2678 - accuracy: 0.6051 - val_loss: 1.1729 - val_accuracy: 0.6340\nEpoch 2/10\n110/110 [==============================] - 12s 111ms/step - loss: 1.0024 - accuracy: 0.6549 - val_loss: 1.1729 - val_accuracy: 0.6553\nEpoch 3/10\n110/110 [==============================] - 12s 112ms/step - loss: 0.9568 - accuracy: 0.6700 - val_loss: 1.3044 - val_accuracy: 0.6347\nEpoch 4/10\n110/110 [==============================] - 12s 111ms/step - loss: 0.8903 - accuracy: 0.6800 - val_loss: 1.1287 - val_accuracy: 0.6347\nEpoch 5/10\n110/110 [==============================] - 12s 112ms/step - loss: 0.8807 - accuracy: 0.6826 - val_loss: 1.7498 - val_accuracy: 0.6360\nEpoch 6/10\n110/110 [==============================] - 12s 112ms/step - loss: 0.8390 - accuracy: 0.6909 - val_loss: 2.2891 - val_accuracy: 0.6427\nEpoch 7/10\n110/110 [==============================] - 12s 111ms/step - loss: 0.8353 - accuracy: 0.6906 - val_loss: 1.6699 - val_accuracy: 0.6467\nEpoch 8/10\n110/110 [==============================] - 12s 112ms/step - loss: 0.8080 - accuracy: 0.7034 - val_loss: 1.4140 - val_accuracy: 0.6427\nEpoch 9/10\n110/110 [==============================] - 12s 112ms/step - loss: 0.7851 - accuracy: 0.7111 - val_loss: 1.8640 - val_accuracy: 0.6260\nEpoch 10/10\n110/110 [==============================] - 12s 112ms/step - loss: 0.7535 - accuracy: 0.7120 - val_loss: 1.7308 - val_accuracy: 0.6267\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer =  RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# predictions need to go through same preprocessings\ndef prep_data(text):\n    text = remove_urls(text)\n    \n    text = remove_non_alphanumeric(text)\n    \n    text = lemmatize_and_remove_stopwords(text)\n\n    tokens = tokenizer.encode_plus(text, max_length=70,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_token_type_ids=False,\n                                   return_tensors='tf')\n\n    return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n            'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}\n\ndef predict(text):\n    in_tensor = prep_data(text)\n    probs = model.predict(in_tensor)[0]\n    return (np.argmax(probs))","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:52:09.119770Z","iopub.execute_input":"2023-10-23T17:52:09.120105Z","iopub.status.idle":"2023-10-23T17:52:09.394917Z","shell.execute_reply.started":"2023-10-23T17:52:09.120078Z","shell.execute_reply":"2023-10-23T17:52:09.394133Z"},"editable":false,"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"predictions = np.asarray(x_test.apply(lambda x: predict(x)))\ntest_vals = np.asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-10-23T17:52:09.396049Z","iopub.execute_input":"2023-10-23T17:52:09.396333Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 3s 3s/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 52ms/step\n1/1 [==============================] - 0s 50ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 49ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 56ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 43ms/step\n1/1 [==============================] - 0s 47ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 55ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 41ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 47ms/step\n1/1 [==============================] - 0s 56ms/step\n1/1 [==============================] - 0s 45ms/step\n1/1 [==============================] - 0s 44ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - ETA: 0s","output_type":"stream"}]},{"cell_type":"code","source":"print(sum)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(test_vals, predictions)\n\n# plot confusion matrix\nsns.set(font_scale=1.0)\nlabels = [\"1\",\"2\",\"3\",\"4\",\"5\"]\nax = sns.heatmap(confusion, annot=True, annot_kws={\"size\": 11}, fmt='d', vmin = 0, cmap='Blues', yticklabels=labels, xticklabels=labels)\nax.set_xlabel('Predicted Class')   \nax.set_ylabel('True Class')   \nax.xaxis.set_label_position('top')\nax.xaxis.tick_top()\nplt.show()\n\n# print classification report\nreport = classification_report(test_vals, predictions, digits = 4, output_dict=False, target_names=[\"bad\", \"neutral\", \"good\"],)\nprint(report)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kf = KFold(n_splits=6)\n# test_preds = []\n# i = 0\n# for train_idx, test_idx in kf.split(X_train[0]):\n#     i += 1\n#     if i not in [1, 5]:  # only do 2 folds to save time\n#         continue\n#     train_split_X = [X_train[i][train_idx] for i in range(len(X_train))]\n#     test_split_X = [X_train[i][test_idx] for i in range(len(X_train))]\n\n#     train_split_y = y_train[train_idx]\n#     test_split_y = y_train[test_idx]\n#     # create class weights to account for imbalance\n#     sentiment_counts = dataset.iloc[train_idx, :].target.value_counts()\n#     total_samples = sentiment_counts.sum()\n\n#     class_weights = {i: total_samples / sentiment_counts[i] for i in range(5)}\n#     class_weights = {key: val / sum(class_weights.values()) for key, val in class_weights.items()}\n\n#     K.clear_session()\n#     config, model, tokenizer = load(learning_rate=2e-5)\n\n#     # fit, test model\n#     model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, class_weight=class_weights, validation_data=(test_split_X, test_split_y))\n\n#     val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n#     val_preds = np.argmax(val_preds, axis=1).flatten()\n#     print(metrics.accuracy_score(y_test[test_idx], val_preds))\n\n#     preds1 = model.predict(X_test, batch_size=32, verbose=1)\n#     test_preds.append(preds1)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}
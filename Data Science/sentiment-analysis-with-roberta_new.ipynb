{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-05T13:52:07.567010Z","iopub.execute_input":"2023-11-05T13:52:07.567478Z","iopub.status.idle":"2023-11-05T13:52:19.765509Z","shell.execute_reply.started":"2023-11-05T13:52:07.567435Z","shell.execute_reply":"2023-11-05T13:52:19.764490Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom transformers import AutoTokenizer, RobertaTokenizer, AdamWeightDecay, TFAutoModel\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:19.767433Z","iopub.execute_input":"2023-11-05T13:52:19.767734Z","iopub.status.idle":"2023-11-05T13:52:29.726261Z","shell.execute_reply.started":"2023-11-05T13:52:19.767710Z","shell.execute_reply":"2023-11-05T13:52:29.725220Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device, True)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:29.727434Z","iopub.execute_input":"2023-11-05T13:52:29.728020Z","iopub.status.idle":"2023-11-05T13:52:29.924454Z","shell.execute_reply.started":"2023-11-05T13:52:29.727993Z","shell.execute_reply":"2023-11-05T13:52:29.922666Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Download NLTK resources\nnltk.download('punkt')  # Download the NLTK resource for tokenization\n\n# Unzip WordNet data file to the NLTK data directory\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n# WordNet is a lexical database for the English language. This command unzips its data to the appropriate NLTK data directory.\n\nnltk.download('wordnet')  # Download the NLTK resource for WordNet\n# WordNet is a large lexical database of English. It's used for various NLP tasks, including lemmatization and synonym/antonym retrieval.\n\nnltk.download('stopwords')  # Download the NLTK resource for stopwords\n# Stopwords are common words (e.g., \"the,\" \"and,\" \"is\") that are often removed from text data when performing natural language processing tasks.\n\nstop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n# This line initializes a set containing common English stopwords, which can be used for text preprocessing.\n\nlemmatizer = WordNetLemmatizer()  # Initialize a WordNet lemmatizer\n# The WordNet lemmatizer is used to reduce words to their base or dictionary form. For example, \"running\" would be lemmatized to \"run.\"\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:29.927316Z","iopub.execute_input":"2023-11-05T13:52:29.927725Z","iopub.status.idle":"2023-11-05T13:52:31.395091Z","shell.execute_reply.started":"2023-11-05T13:52:29.927684Z","shell.execute_reply":"2023-11-05T13:52:31.393926Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset=pd.read_csv(\"/kaggle/input/cumulative-reviews-and-ratings/cumulative.csv\")\ndataset = dataset[dataset['rating'] != '|']\n#make 100000 instead if 10000\ndataset = dataset.sample(n=10000, random_state=42) #Taking a sample for training, otherwise kernel crashes, ig for offline implementation, this shouldn't be an issue","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:31.396576Z","iopub.execute_input":"2023-11-05T13:52:31.396856Z","iopub.status.idle":"2023-11-05T13:52:40.422686Z","shell.execute_reply.started":"2023-11-05T13:52:31.396831Z","shell.execute_reply":"2023-11-05T13:52:40.421715Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/1344748012.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  dataset=pd.read_csv(\"/kaggle/input/cumulative-reviews-and-ratings/cumulative.csv\")\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(dataset['review'].iloc[0]))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:40.425033Z","iopub.execute_input":"2023-11-05T13:52:40.425421Z","iopub.status.idle":"2023-11-05T13:52:40.430401Z","shell.execute_reply.started":"2023-11-05T13:52:40.425396Z","shell.execute_reply":"2023-11-05T13:52:40.429394Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['review'] = dataset['review'].str.replace('&', 'and')\n\ndef remove_urls(text):\n    # Regular expression to match URLs\n    url_pattern = r'https?://\\S+|www\\.\\S+'\n    return re.sub(url_pattern, '', text)\n\n# Apply the remove_urls function to the 'review' column\ndataset['review'] = dataset['review'].apply(lambda x: remove_urls(x))\n\ndef remove_non_alphanumeric(text):\n    return re.sub(r'[^a-zA-Z0-9.]', ' ', text)\n\n# Apply the remove_non_alphanumeric function to the \"reviews\" column\ndataset['review'] = dataset['review'].apply(remove_non_alphanumeric)\n\nprint(list(dataset.columns))\nprint(dataset.head())\nprint(len(dataset))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:40.431702Z","iopub.execute_input":"2023-11-05T13:52:40.431994Z","iopub.status.idle":"2023-11-05T13:52:40.952956Z","shell.execute_reply.started":"2023-11-05T13:52:40.431969Z","shell.execute_reply":"2023-11-05T13:52:40.951975Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['rating', 'review']\n       rating                                             review\n303990      5  My son and I love these beef sticks  I purchas...\n269665      5  Since the stores do not carry this product  I ...\n380357      5  My cat loves all the Weruva varieties. I rotat...\n37894       5  I can have about 3 of these a week as a protei...\n444547      5  HAVE BEEN USING THE CROWN PRICE LUMP WHITE CRA...\n10000\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset['rating'] = dataset['rating'].astype(float).round() #converting ratings to numerical values\ndataset['review'] = dataset['review'].str.lower() #uniformly making all letters lower case in the reviews\n\nprint(list(dataset.columns))\nprint(dataset.head())\nprint(len(dataset))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:40.954503Z","iopub.execute_input":"2023-11-05T13:52:40.954776Z","iopub.status.idle":"2023-11-05T13:52:40.972676Z","shell.execute_reply.started":"2023-11-05T13:52:40.954753Z","shell.execute_reply":"2023-11-05T13:52:40.971778Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['rating', 'review']\n        rating                                             review\n303990     5.0  my son and i love these beef sticks  i purchas...\n269665     5.0  since the stores do not carry this product  i ...\n380357     5.0  my cat loves all the weruva varieties. i rotat...\n37894      5.0  i can have about 3 of these a week as a protei...\n444547     5.0  have been using the crown price lump white cra...\n10000\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef lemmatize_and_remove_stopwords(text):\n    words = nltk.word_tokenize(text) \n    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    return ' '.join(filtered_words)\n\n# Apply the lemmatize_and_remove_stopwords function to the \"reviews\" column\ndataset['review'] = dataset['review'].apply(lemmatize_and_remove_stopwords)\n\nprint(list(dataset.columns))\nprint(dataset.head())\nprint(len(dataset))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:40.974039Z","iopub.execute_input":"2023-11-05T13:52:40.974822Z","iopub.status.idle":"2023-11-05T13:52:55.441525Z","shell.execute_reply.started":"2023-11-05T13:52:40.974792Z","shell.execute_reply":"2023-11-05T13:52:55.440497Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"['rating', 'review']\n        rating                                             review\n303990     5.0  son love beef stick purchased slim jims time a...\n269665     5.0  since store carry product lucky come across li...\n380357     5.0  cat love weruva variety . rotate wellness wet ...\n37894      5.0  3 week protein meal snack anytime feeling hung...\n444547     5.0  using crown price lump white crab meat years.i...\n10000\n","output_type":"stream"}]},{"cell_type":"code","source":"q=list(dataset['review'].isna())\nprint(sum(q))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.446986Z","iopub.execute_input":"2023-11-05T13:52:55.447304Z","iopub.status.idle":"2023-11-05T13:52:55.454504Z","shell.execute_reply.started":"2023-11-05T13:52:55.447278Z","shell.execute_reply":"2023-11-05T13:52:55.453673Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"}]},{"cell_type":"code","source":"# #LOOK INTO THIS MAX_LENGTH AND WHAT ARE THE PARAMETERS I AM USING\n# def to_tokens(input_text, tokenizer):\n#     output = tokenizer.encode_plus(text=input_text, max_length=90, pad_to_max_length=True, truncation=True)\n# #     print(\"to_tokens:\", type(output))\n# #     print(output.keys())\n#     return output\n\n# def select_field(features, field):\n#     return [feature[field] for feature in features]","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.455698Z","iopub.execute_input":"2023-11-05T13:52:55.456069Z","iopub.status.idle":"2023-11-05T13:52:55.467740Z","shell.execute_reply.started":"2023-11-05T13:52:55.456045Z","shell.execute_reply":"2023-11-05T13:52:55.466811Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# string=\"I love NLP, I need to know more about it. He is happy where he is.\"\n# print(string)\n# string = remove_non_alphanumeric(string)\n# print(string)\n# string = lemmatize_and_remove_stopwords(string)\n# string=string.lower()\n# print(string)\n\n# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n\n# out = to_tokens(string,tokenizer)\n\n# print(len(out['input_ids']),len(out['attention_mask']))\n# print(out['input_ids'])\n# print(out['attention_mask'])\n\n# print(tokenizer.convert_ids_to_tokens(out['input_ids']))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.468917Z","iopub.execute_input":"2023-11-05T13:52:55.469358Z","iopub.status.idle":"2023-11-05T13:52:55.481002Z","shell.execute_reply.started":"2023-11-05T13:52:55.469322Z","shell.execute_reply":"2023-11-05T13:52:55.480012Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"input_text = \"I love natural language processing\"\n\nsimplified output =\n{\n    'input_ids': [0, 101, 1045, 2293, 3015, 2652, 3836, 102, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n    'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n}","metadata":{}},{"cell_type":"code","source":"# def preprocess_data(tokenizer):\n#     data_encoded = dataset['review'].apply(lambda x: to_tokens(x, tokenizer))\n\n#     # Create attention masks\n#     input_ids = np.array(select_field(data_encoded, 'input_ids'))\n#     attention_masks = np.array(select_field(data_encoded, 'attention_mask'))\n    \n#     # Manually split data into train and test sets\n#     split_index = int(len(data_encoded) * 0.7)\n#     X_train_input_ids, X_test_input_ids = input_ids[:split_index], input_ids[split_index:]\n#     X_train_attention_masks, X_test_attention_masks = attention_masks[:split_index], attention_masks[split_index:]\n    \n#     X_train = np.array([X_train_input_ids, X_train_attention_masks])\n#     X_test = np.array([X_test_input_ids, X_test_attention_masks])\n    \n#     y_train = np.array(dataset['rating'].iloc[:split_index])\n#     y_test = np.array(dataset['rating'].iloc[split_index:])\n    \n#     return X_train, X_test, y_train, y_test\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.482238Z","iopub.execute_input":"2023-11-05T13:52:55.482503Z","iopub.status.idle":"2023-11-05T13:52:55.496143Z","shell.execute_reply.started":"2023-11-05T13:52:55.482481Z","shell.execute_reply":"2023-11-05T13:52:55.495208Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def load(learning_rate,epsilon=1e-8,num_labels=5,task=\"sentiment_analysis\",model_name='roberta-large'):\n    config_class, model_class, tokenizer_class= RobertaConfig, TFRobertaForSequenceClassification, RobertaTokenizer\n\n    config = config_class.from_pretrained(model_name, num_labels=num_labels, finetuning_task=\"multi_class\")\n\n\n    model_metrics = [\n        keras.metrics.TruePositives(name='tp'),\n        keras.metrics.FalsePositives(name='fp'), \n        keras.metrics.TrueNegatives(name='tn'),\n        keras.metrics.FalseNegatives(name='fn'), \n        keras.metrics.BinaryAccuracy(name='accuracy'),\n        keras.metrics.Precision(name='precision'),\n        keras.metrics.Recall(name='recall'),\n        keras.metrics.AUC(name='auc'),\n    ]\n\n    model = model_class.from_pretrained(model_name)\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon, clipnorm=1.0)\n    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n    metric = keras.metrics.CategoricalAccuracy('accuracy')\n\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n\n    tokenizer = tokenizer_class.from_pretrained(model_name, lower_case = False)\n    return config, model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.497198Z","iopub.execute_input":"2023-11-05T13:52:55.497456Z","iopub.status.idle":"2023-11-05T13:52:55.508010Z","shell.execute_reply.started":"2023-11-05T13:52:55.497434Z","shell.execute_reply":"2023-11-05T13:52:55.507222Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# config, model, tokenizer = load(learning_rate=2e-5)\n# X_train,X_test,y_train,y_test=preprocess_data(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.509064Z","iopub.execute_input":"2023-11-05T13:52:55.509358Z","iopub.status.idle":"2023-11-05T13:52:55.523926Z","shell.execute_reply.started":"2023-11-05T13:52:55.509335Z","shell.execute_reply":"2023-11-05T13:52:55.522951Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train = dataset.iloc[:int(dataset.shape[0] * 0.7)]\ntest = dataset.iloc[int(dataset.shape[0] * 0.7):]\nprint(train.shape,test.shape)\nx_train,y_train = train['review'],train['rating']\nx_test, y_test = test['review'],test['rating']\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.525000Z","iopub.execute_input":"2023-11-05T13:52:55.525278Z","iopub.status.idle":"2023-11-05T13:52:55.537016Z","shell.execute_reply.started":"2023-11-05T13:52:55.525254Z","shell.execute_reply":"2023-11-05T13:52:55.536063Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(7000, 2) (3000, 2)\n(7000,) (7000,)\n(3000,) (3000,)\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer =  RobertaTokenizer.from_pretrained(\"roberta-base\")\n\ndef tokenize(df):\n    input_ids =  []\n    attention_masks =  []\n    \n    for i, text in enumerate(df):\n        tokens = tokenizer.encode_plus(text, max_length=70,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_attention_mask=True,\n                                   return_token_type_ids=False, return_tensors='tf')\n         \n        input_ids.append(np.asarray(tokens[\"input_ids\"]).reshape(70,))\n        attention_masks.append(np.asarray(tokens[\"attention_mask\"]).reshape(70,))\n\n    return (np.asarray(input_ids), np.asarray(attention_masks))\n\nx_train_input_ids, x_train_attention_masks = tokenize(x_train)\nx_test_input_ids, x_test_attention_masks = tokenize(x_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:52:55.538261Z","iopub.execute_input":"2023-11-05T13:52:55.538546Z","iopub.status.idle":"2023-11-05T13:53:06.977500Z","shell.execute_reply.started":"2023-11-05T13:52:55.538515Z","shell.execute_reply":"2023-11-05T13:53:06.976564Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cb20c968a1047bfbbef0f98f05352d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3154854c03f942599d92b14fa1a8d8c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546bbc5477f2455fb8af6a2bf92bd5a7"}},"metadata":{}}]},{"cell_type":"code","source":"print(x_train_input_ids.shape, x_train_attention_masks.shape)\nprint(x_test_input_ids.shape, x_test_attention_masks.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:06.980268Z","iopub.execute_input":"2023-11-05T13:53:06.980670Z","iopub.status.idle":"2023-11-05T13:53:06.987742Z","shell.execute_reply.started":"2023-11-05T13:53:06.980631Z","shell.execute_reply":"2023-11-05T13:53:06.986519Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"(7000, 70) (7000, 70)\n(3000, 70) (3000, 70)\n","output_type":"stream"}]},{"cell_type":"code","source":"def one_encode_labels(df):\n    sentiment_values = set(dataset[\"rating\"].values)\n    labels = []\n    for index, row in df.iterrows():\n        label = np.zeros((len(sentiment_values)))\n        label[row[\"sentiment\"]] = 1 \n        labels.append(label)\n    labels = np.asarray(labels)\n    return labels","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:06.989627Z","iopub.execute_input":"2023-11-05T13:53:06.989985Z","iopub.status.idle":"2023-11-05T13:53:07.001575Z","shell.execute_reply.started":"2023-11-05T13:53:06.989952Z","shell.execute_reply":"2023-11-05T13:53:07.000555Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(y_train,y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:07.002786Z","iopub.execute_input":"2023-11-05T13:53:07.003301Z","iopub.status.idle":"2023-11-05T13:53:07.018373Z","shell.execute_reply.started":"2023-11-05T13:53:07.003274Z","shell.execute_reply":"2023-11-05T13:53:07.017447Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"303990    5.0\n269665    5.0\n380357    5.0\n37894     5.0\n444547    5.0\n         ... \n64465     1.0\n61725     4.0\n160540    5.0\n345485    2.0\n8700      5.0\nName: rating, Length: 7000, dtype: float64 88638     1.0\n242005    5.0\n12029     5.0\n230383    1.0\n230623    4.0\n         ... \n419306    5.0\n121499    5.0\n259270    4.0\n294861    1.0\n251404    4.0\nName: rating, Length: 3000, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"print(y_train)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:07.019425Z","iopub.execute_input":"2023-11-05T13:53:07.019663Z","iopub.status.idle":"2023-11-05T13:53:07.032109Z","shell.execute_reply.started":"2023-11-05T13:53:07.019641Z","shell.execute_reply":"2023-11-05T13:53:07.031247Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"303990    5.0\n269665    5.0\n380357    5.0\n37894     5.0\n444547    5.0\n         ... \n64465     1.0\n61725     4.0\n160540    5.0\n345485    2.0\n8700      5.0\nName: rating, Length: 7000, dtype: float64\n","output_type":"stream"}]},{"cell_type":"code","source":"def one_encode_labels(df):\n    sentiment_values = set(df)\n    labels = []\n    for rating in df:\n        label = np.zeros((len(sentiment_values)))\n        if rating == 1.0:\n            label[0]=1\n        elif rating == 2.0:\n            label[1] =1\n        elif rating ==3.0:\n            label[2]=1\n        elif rating == 4.0:\n            label[3]=1\n        elif rating == 5.0:\n            label[4] =1\n        labels.append(label)\n    labels = np.asarray(labels)\n    return labels\n\ny_train = one_encode_labels(y_train)\ny_test = one_encode_labels(y_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:07.033115Z","iopub.execute_input":"2023-11-05T13:53:07.033441Z","iopub.status.idle":"2023-11-05T13:53:07.067028Z","shell.execute_reply.started":"2023-11-05T13:53:07.033417Z","shell.execute_reply":"2023-11-05T13:53:07.066210Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"roberta = TFAutoModel.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:07.068269Z","iopub.execute_input":"2023-11-05T13:53:07.068601Z","iopub.status.idle":"2023-11-05T13:53:19.591335Z","shell.execute_reply.started":"2023-11-05T13:53:07.068561Z","shell.execute_reply":"2023-11-05T13:53:19.590552Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26536e57b00943ddb668d7a18f8b9085"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'roberta.embeddings.position_ids', 'lm_head.layer_norm.weight']\n- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = tf.keras.layers.Input(shape=(70,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(70,), name='attention_mask', dtype='int32')\n\nembeddings = roberta(input_ids, attention_mask=mask)[0]\n\nX = tf.keras.layers.LSTM(128)(embeddings)\nX = tf.keras.layers.BatchNormalization()(X)\nX = tf.keras.layers.Dense(768)(X)\nX = tf.keras.layers.Activation(\"relu\")(X)\nX = tf.keras.layers.Dense(768)(X)\nX = tf.keras.layers.Dropout(0.1)(X)\ny = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(X)\n\nmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\nmodel.layers[2].trainable = False\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:19.592768Z","iopub.execute_input":"2023-11-05T13:53:19.593396Z","iopub.status.idle":"2023-11-05T13:53:26.463175Z","shell.execute_reply.started":"2023-11-05T13:53:19.593358Z","shell.execute_reply":"2023-11-05T13:53:26.462213Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_ids (InputLayer)         [(None, 70)]         0           []                               \n                                                                                                  \n attention_mask (InputLayer)    [(None, 70)]         0           []                               \n                                                                                                  \n tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  124645632  ['input_ids[0][0]',              \n el)                            thPoolingAndCrossAt               'attention_mask[0][0]']         \n                                tentions(last_hidde                                               \n                                n_state=(None, 70,                                                \n                                768),                                                             \n                                 pooler_output=(Non                                               \n                                e, 768),                                                          \n                                 past_key_values=No                                               \n                                ne, hidden_states=N                                               \n                                one, attentions=Non                                               \n                                e, cross_attentions                                               \n                                =None)                                                            \n                                                                                                  \n lstm (LSTM)                    (None, 128)          459264      ['tf_roberta_model[0][0]']       \n                                                                                                  \n batch_normalization (BatchNorm  (None, 128)         512         ['lstm[0][0]']                   \n alization)                                                                                       \n                                                                                                  \n dense (Dense)                  (None, 768)          99072       ['batch_normalization[0][0]']    \n                                                                                                  \n activation (Activation)        (None, 768)          0           ['dense[0][0]']                  \n                                                                                                  \n dense_1 (Dense)                (None, 768)          590592      ['activation[0][0]']             \n                                                                                                  \n dropout_37 (Dropout)           (None, 768)          0           ['dense_1[0][0]']                \n                                                                                                  \n outputs (Dense)                (None, 5)            3845        ['dropout_37[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 125,798,917\nTrainable params: 1,153,029\nNon-trainable params: 124,645,888\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = AdamWeightDecay(2e-03, beta_1=0.8, beta_2=0.9, weight_decay_rate=0.0001)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=[acc])","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:26.464626Z","iopub.execute_input":"2023-11-05T13:53:26.465459Z","iopub.status.idle":"2023-11-05T13:53:26.490641Z","shell.execute_reply.started":"2023-11-05T13:53:26.465421Z","shell.execute_reply":"2023-11-05T13:53:26.489740Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(x_train_input_ids.shape, x_train_attention_masks.shape)\nprint(x_test_input_ids.shape,x_test_attention_masks.shape)\nprint(y_train.shape,y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:26.491792Z","iopub.execute_input":"2023-11-05T13:53:26.492428Z","iopub.status.idle":"2023-11-05T13:53:26.497611Z","shell.execute_reply.started":"2023-11-05T13:53:26.492393Z","shell.execute_reply":"2023-11-05T13:53:26.496741Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(7000, 70) (7000, 70)\n(3000, 70) (3000, 70)\n(7000, 5) (3000, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n# assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:26.498862Z","iopub.execute_input":"2023-11-05T13:53:26.499221Z","iopub.status.idle":"2023-11-05T13:53:26.507402Z","shell.execute_reply.started":"2023-11-05T13:53:26.499189Z","shell.execute_reply":"2023-11-05T13:53:26.506531Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:26.512628Z","iopub.execute_input":"2023-11-05T13:53:26.513153Z","iopub.status.idle":"2023-11-05T13:53:26.517916Z","shell.execute_reply.started":"2023-11-05T13:53:26.513127Z","shell.execute_reply":"2023-11-05T13:53:26.516990Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"history = model.fit((x_train_input_ids, x_train_attention_masks), y_train, validation_data=((x_test_input_ids, x_test_attention_masks), y_test), epochs=50,callbacks=[early_stopping])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T14:27:43.137881Z","iopub.execute_input":"2023-11-05T14:27:43.138586Z","iopub.status.idle":"2023-11-05T14:36:19.924095Z","shell.execute_reply.started":"2023-11-05T14:27:43.138550Z","shell.execute_reply":"2023-11-05T14:36:19.923175Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch 1/50\n219/219 [==============================] - 53s 171ms/step - loss: 1.1381 - accuracy: 0.6327 - val_loss: 1.1249 - val_accuracy: 0.6513\nEpoch 2/50\n219/219 [==============================] - 33s 151ms/step - loss: 0.9494 - accuracy: 0.6646 - val_loss: 1.0021 - val_accuracy: 0.6713\nEpoch 3/50\n219/219 [==============================] - 33s 150ms/step - loss: 0.9098 - accuracy: 0.6731 - val_loss: 1.4311 - val_accuracy: 0.6620\nEpoch 4/50\n219/219 [==============================] - 33s 152ms/step - loss: 0.8752 - accuracy: 0.6817 - val_loss: 0.9141 - val_accuracy: 0.6723\nEpoch 5/50\n219/219 [==============================] - 33s 152ms/step - loss: 0.8520 - accuracy: 0.6867 - val_loss: 1.1495 - val_accuracy: 0.6760\nEpoch 6/50\n219/219 [==============================] - 33s 151ms/step - loss: 0.8422 - accuracy: 0.6907 - val_loss: 1.3350 - val_accuracy: 0.6190\nEpoch 7/50\n219/219 [==============================] - 33s 149ms/step - loss: 0.8325 - accuracy: 0.6967 - val_loss: 1.1598 - val_accuracy: 0.6723\nEpoch 8/50\n219/219 [==============================] - 33s 150ms/step - loss: 0.8089 - accuracy: 0.7009 - val_loss: 1.3962 - val_accuracy: 0.6663\nEpoch 9/50\n219/219 [==============================] - 33s 151ms/step - loss: 0.7885 - accuracy: 0.7059 - val_loss: 1.0647 - val_accuracy: 0.6573\nEpoch 10/50\n219/219 [==============================] - 34s 154ms/step - loss: 0.7649 - accuracy: 0.7143 - val_loss: 1.5200 - val_accuracy: 0.6837\nEpoch 11/50\n219/219 [==============================] - 33s 152ms/step - loss: 0.7547 - accuracy: 0.7167 - val_loss: 1.1845 - val_accuracy: 0.6667\nEpoch 12/50\n219/219 [==============================] - 33s 152ms/step - loss: 0.7361 - accuracy: 0.7236 - val_loss: 1.3033 - val_accuracy: 0.6140\nEpoch 13/50\n219/219 [==============================] - 33s 152ms/step - loss: 0.7081 - accuracy: 0.7326 - val_loss: 1.3429 - val_accuracy: 0.6627\nEpoch 14/50\n219/219 [==============================] - 33s 152ms/step - loss: 0.6897 - accuracy: 0.7390 - val_loss: 1.3538 - val_accuracy: 0.6640\nEpoch 15/50\n219/219 [==============================] - 34s 154ms/step - loss: 0.6680 - accuracy: 0.7456 - val_loss: 1.7260 - val_accuracy: 0.6757\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer =  RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# predictions need to go through same preprocessings\ndef prep_data(text):\n    text = remove_urls(text)\n    \n    text = remove_non_alphanumeric(text)\n    \n    text = lemmatize_and_remove_stopwords(text)\n\n    tokens = tokenizer.encode_plus(text, max_length=70,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_token_type_ids=False,\n                                   return_tensors='tf')\n\n    return {'input_ids': tf.cast(tokens['input_ids'], tf.float64),\n            'attention_mask': tf.cast(tokens['attention_mask'], tf.float64)}\n\ndef predict(text):\n    in_tensor = prep_data(text)\n    probs = model.predict(in_tensor)[0]\n    return (np.argmax(probs))","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:27.738237Z","iopub.status.idle":"2023-11-05T13:53:27.738570Z","shell.execute_reply.started":"2023-11-05T13:53:27.738409Z","shell.execute_reply":"2023-11-05T13:53:27.738424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = np.asarray(x_test.apply(lambda x: predict(x)))\ntest_vals = np.asarray(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:27.740089Z","iopub.status.idle":"2023-11-05T13:53:27.740443Z","shell.execute_reply.started":"2023-11-05T13:53:27.740280Z","shell.execute_reply":"2023-11-05T13:53:27.740297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(sum)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:27.741886Z","iopub.status.idle":"2023-11-05T13:53:27.742249Z","shell.execute_reply.started":"2023-11-05T13:53:27.742063Z","shell.execute_reply":"2023-11-05T13:53:27.742085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(test_vals, predictions)\n\n# plot confusion matrix\nsns.set(font_scale=1.0)\nlabels = [\"1\",\"2\",\"3\",\"4\",\"5\"]\nax = sns.heatmap(confusion, annot=True, annot_kws={\"size\": 11}, fmt='d', vmin = 0, cmap='Blues', yticklabels=labels, xticklabels=labels)\nax.set_xlabel('Predicted Class')   \nax.set_ylabel('True Class')   \nax.xaxis.set_label_position('top')\nax.xaxis.tick_top()\nplt.show()\n\n# print classification report\nreport = classification_report(test_vals, predictions, digits = 4, output_dict=False, target_names=[\"bad\", \"neutral\", \"good\"],)\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:27.743580Z","iopub.status.idle":"2023-11-05T13:53:27.743944Z","shell.execute_reply.started":"2023-11-05T13:53:27.743763Z","shell.execute_reply":"2023-11-05T13:53:27.743780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kf = KFold(n_splits=6)\n# test_preds = []\n# i = 0\n# for train_idx, test_idx in kf.split(X_train[0]):\n#     i += 1\n#     if i not in [1, 5]:  # only do 2 folds to save time\n#         continue\n#     train_split_X = [X_train[i][train_idx] for i in range(len(X_train))]\n#     test_split_X = [X_train[i][test_idx] for i in range(len(X_train))]\n\n#     train_split_y = y_train[train_idx]\n#     test_split_y = y_train[test_idx]\n#     # create class weights to account for imbalance\n#     sentiment_counts = dataset.iloc[train_idx, :].target.value_counts()\n#     total_samples = sentiment_counts.sum()\n\n#     class_weights = {i: total_samples / sentiment_counts[i] for i in range(5)}\n#     class_weights = {key: val / sum(class_weights.values()) for key, val in class_weights.items()}\n\n#     K.clear_session()\n#     config, model, tokenizer = load(learning_rate=2e-5)\n\n#     # fit, test model\n#     model.fit(train_split_X, train_split_y, batch_size=64, epochs=3, class_weight=class_weights, validation_data=(test_split_X, test_split_y))\n\n#     val_preds = model.predict(test_split_X, batch_size=32, verbose=1)\n#     val_preds = np.argmax(val_preds, axis=1).flatten()\n#     print(metrics.accuracy_score(y_test[test_idx], val_preds))\n\n#     preds1 = model.predict(X_test, batch_size=32, verbose=1)\n#     test_preds.append(preds1)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:53:27.745441Z","iopub.status.idle":"2023-11-05T13:53:27.745899Z","shell.execute_reply.started":"2023-11-05T13:53:27.745668Z","shell.execute_reply":"2023-11-05T13:53:27.745690Z"},"trusted":true},"execution_count":null,"outputs":[]}]}